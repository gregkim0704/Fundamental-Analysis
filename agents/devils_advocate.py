"""Devil's Advocate Agent - ë°˜ëŒ€ ë…¼ê±° ì „ë¬¸ê°€."""
import json
import logging
from typing import Any, Optional

from agents.base_agent import BaseAgent, AgentConfig
from models.agent_opinion import AgentType, RiskLevel, Sentiment
from models.analysis_result import DevilsAdvocateAnalysis

logger = logging.getLogger(__name__)


class DevilsAdvocateAgent(BaseAgent):
    """Devil's Advocate agent - challenges other agents' analyses."""

    def __init__(self, config: Optional[AgentConfig] = None):
        """Initialize Devil's Advocate Agent."""
        if config is None:
            config = AgentConfig(
                agent_type=AgentType.DEVILS_ADVOCATE,
                prompt_file="prompts/devils_advocate.md",
                temperature=0.5,  # Slightly higher for more creative challenges
            )
        super().__init__(config)

    def _get_default_prompt(self) -> str:
        """Get default system prompt."""
        return """You are the Devil's Advocate. Your role is to intentionally challenge
and question other agents' analyses to identify blind spots and weaknesses.

Core questions to ask:
1. "Why is it at this price?" - What's the market's logic?
2. "Is this already known?" - Do we have an information edge?
3. "What's the worst case?" - Pre-mortem analysis
4. "What historical precedents exist?" - Similar failure cases

Be constructively critical, not just negative. Provide evidence-based counter-arguments."""

    async def analyze(self, context: dict[str, Any]) -> dict[str, Any]:
        """Generate counter-arguments to other agents' analyses.

        Args:
            context: Analysis context including all other agent results

        Returns:
            Devil's advocate analysis result
        """
        ticker = context.get("ticker", "")
        current_price = context.get("current_price", 0)

        # Get other agents' analyses
        macro_analysis = context.get("macro_analysis", {})
        quant_analysis = context.get("quant_analysis", {})
        qualitative_analysis = context.get("qualitative_analysis", {})
        industry_analysis = context.get("industry_analysis", {})
        valuation_analysis = context.get("valuation_analysis", {})

        # Build analysis prompt
        prompt = f"""As Devil's Advocate, challenge the following investment analyses for {ticker}:

Current Price: {current_price}

=== MACRO ANALYSIS ===
{json.dumps(macro_analysis, indent=2, default=str)}

=== QUANT ANALYSIS ===
{json.dumps(quant_analysis, indent=2, default=str)}

=== QUALITATIVE ANALYSIS ===
{json.dumps(qualitative_analysis, indent=2, default=str)}

=== INDUSTRY ANALYSIS ===
{json.dumps(industry_analysis, indent=2, default=str)}

=== VALUATION ANALYSIS ===
{json.dumps(valuation_analysis, indent=2, default=str)}

Provide your devil's advocate analysis in JSON format:
{{
    "score": <1-10, where 10 = most concerns>,
    "confidence": <0-100>,
    "sentiment": "<very_bullish|bullish|neutral|bearish|very_bearish>",
    "summary": "<1-2 sentence summary of key concerns>",
    "why_this_price": "<analysis of why current price exists>",
    "what_market_knows": "<what's already priced in>",
    "potential_blind_spots": ["<blind_spot1>", "<blind_spot2>"],
    "macro_counter_arguments": ["<counter1>", "<counter2>"],
    "quant_counter_arguments": ["<counter1>", "<counter2>"],
    "qualitative_counter_arguments": ["<counter1>", "<counter2>"],
    "industry_counter_arguments": ["<counter1>", "<counter2>"],
    "valuation_counter_arguments": ["<counter1>", "<counter2>"],
    "pre_mortem_scenarios": [
        "<scenario where investment fails 1>",
        "<scenario where investment fails 2>"
    ],
    "historical_precedents": ["<similar failure case>"],
    "red_flags": ["<red flag 1>", "<red flag 2>"],
    "warning_signals": ["<signal to monitor 1>", "<signal to monitor 2>"],
    "key_points": ["<key concern 1>", "<key concern 2>"],
    "concerns": ["<serious concern 1>", "<serious concern 2>"],
    "questions_to_answer": ["<question needing verification>"]
}}"""

        try:
            response = await self.invoke(prompt)
            result = self._parse_json_response(response)

            # Create DevilsAdvocateAnalysis object
            analysis = DevilsAdvocateAnalysis(
                score=result.get("score", 5),
                sentiment=self._map_sentiment(result.get("sentiment", "neutral")),
                summary=result.get("summary", ""),
                why_this_price=result.get("why_this_price", ""),
                what_market_knows=result.get("what_market_knows", ""),
                potential_blind_spots=result.get("potential_blind_spots", []),
                macro_counter_arguments=result.get("macro_counter_arguments", []),
                quant_counter_arguments=result.get("quant_counter_arguments", []),
                qualitative_counter_arguments=result.get("qualitative_counter_arguments", []),
                industry_counter_arguments=result.get("industry_counter_arguments", []),
                valuation_counter_arguments=result.get("valuation_counter_arguments", []),
                pre_mortem_scenarios=result.get("pre_mortem_scenarios", []),
                historical_precedents=result.get("historical_precedents", []),
                red_flags=result.get("red_flags", []),
                warning_signals=result.get("warning_signals", []),
            )

            return {
                "analysis": analysis.model_dump(),
                "raw_result": result,
                "opinion": self.create_opinion(
                    ticker=ticker,
                    score=10 - result.get("score", 5),  # Invert score for opinion
                    confidence=result.get("confidence", 75),
                    summary=result.get("summary", ""),
                    key_points=result.get("key_points", []),
                    concerns=result.get("concerns", []),
                    detailed_analysis=result,
                ).model_dump(),
            }

        except Exception as e:
            logger.error(f"Error in devil's advocate analysis: {e}")
            return {
                "error": str(e),
                "analysis": None,
            }

    async def generate_counter_argument(
        self,
        agent_type: str,
        original_claim: str,
        evidence: Optional[list[str]] = None,
    ) -> dict[str, Any]:
        """Generate a specific counter-argument to a claim.

        Args:
            agent_type: Type of agent making the claim
            original_claim: The claim to challenge
            evidence: Optional supporting evidence for the claim

        Returns:
            Counter-argument details
        """
        prompt = f"""Generate a counter-argument to the following claim:

Agent: {agent_type}
Claim: {original_claim}
Supporting Evidence: {json.dumps(evidence or [])}

Provide your counter-argument in JSON format:
{{
    "counter_argument": "<your counter-argument>",
    "severity": "<low|medium|high|critical>",
    "evidence_against": ["<evidence1>", "<evidence2>"],
    "questions_raised": ["<question1>", "<question2>"]
}}"""

        try:
            response = await self.invoke(prompt)
            return self._parse_json_response(response)
        except Exception as e:
            logger.error(f"Error generating counter-argument: {e}")
            return {
                "counter_argument": "Unable to generate counter-argument",
                "severity": "low",
                "evidence_against": [],
                "questions_raised": [],
            }

    def _parse_json_response(self, response: str) -> dict[str, Any]:
        """Parse JSON from LLM response."""
        try:
            return json.loads(response)
        except json.JSONDecodeError:
            if "```json" in response:
                json_str = response.split("```json")[1].split("```")[0]
                return json.loads(json_str)
            elif "```" in response:
                json_str = response.split("```")[1].split("```")[0]
                return json.loads(json_str)
            else:
                return {
                    "score": 5,
                    "confidence": 50,
                    "sentiment": "neutral",
                    "summary": response[:200],
                    "key_points": [],
                    "concerns": [],
                }

    def _map_sentiment(self, sentiment_str: str) -> Sentiment:
        """Map string sentiment to Sentiment enum."""
        mapping = {
            "very_bullish": Sentiment.VERY_BULLISH,
            "bullish": Sentiment.BULLISH,
            "neutral": Sentiment.NEUTRAL,
            "bearish": Sentiment.BEARISH,
            "very_bearish": Sentiment.VERY_BEARISH,
        }
        return mapping.get(sentiment_str.lower(), Sentiment.NEUTRAL)

    async def challenge(self, context: dict[str, Any]) -> dict[str, Any]:
        """í† ë¡ ì—ì„œ íŠ¹ì • ì—ì´ì „íŠ¸ì˜ ë¶„ì„ì— ë„ì „í•©ë‹ˆë‹¤.

        Args:
            context:
                - target_agent: ë„ì „ ëŒ€ìƒ ì—ì´ì „íŠ¸
                - target_score: ëŒ€ìƒì˜ ì ìˆ˜
                - target_sentiment: ëŒ€ìƒì˜ sentiment
                - target_summary: ëŒ€ìƒì˜ ë¶„ì„ ìš”ì•½
                - target_key_points: ëŒ€ìƒì˜ í•µì‹¬ í¬ì¸íŠ¸
                - round_number: í† ë¡  ë¼ìš´ë“œ ë²ˆí˜¸
                - stock_info: ì£¼ì‹ ì •ë³´

        Returns:
            ë„ì „ ë‚´ìš©:
                - counter_argument: ë°˜ë°• ë…¼ë¦¬
                - evidence: ì¦ê±°
                - severity: ì‹¬ê°ë„
                - questions: ë‹µí•´ì•¼ í•  ì§ˆë¬¸ë“¤
        """
        target_agent = context.get("target_agent", "unknown")
        target_score = context.get("target_score", 5)
        target_summary = context.get("target_summary", "")
        target_key_points = context.get("target_key_points", [])
        round_number = context.get("round_number", 1)
        stock_info = context.get("stock_info", {})

        # ì‚¬ìš©ì ì œê³µ ìë£Œ ì»¨í…ìŠ¤íŠ¸
        user_research = context.get("user_research", {})
        research_synthesis = context.get("research_synthesis", {})

        # ë¼ìš´ë“œë³„ë¡œ ë„ì „ ê°•ë„ ì¡°ì ˆ
        intensity = ["moderate", "aggressive", "final_verification"][min(round_number - 1, 2)]

        # ì‚¬ìš©ì ìë£Œ ì„¹ì…˜ ìƒì„±
        user_research_section = ""
        if user_research and user_research.get("documents"):
            user_research_section = f"""
## ğŸ“š ì‚¬ìš©ì ì œê³µ ìë£Œ (ì°¸ì¡°ìš©)
- ì´ {user_research.get('total_documents', 0)}ê°œ ìë£Œ
- í‰ê·  ì‹ ë¢°ë„: {user_research.get('avg_reliability', 'N/A')}/10
- ì‹ ë¢°ë„ ìˆ˜ì¤€: {user_research.get('reliability_level', 'unknown')}

### ìë£Œ ìš”ì•½
{json.dumps(user_research.get('documents', [])[:5], ensure_ascii=False, indent=2)}

### ì‚¬ìš©ì ê°€ì„¤
{user_research.get('user_hypothesis', 'ì—†ìŒ')}

### ì‚¬ìš©ì ìš°ë ¤ì‚¬í•­
{json.dumps(user_research.get('user_concerns', []), ensure_ascii=False)}

### âš ï¸ í¸í–¥ ê²½ê³  (ë†’ì€ í¸í–¥ ìë£Œ)
{json.dumps(user_research.get('bias_warnings', []), ensure_ascii=False, indent=2) if user_research.get('bias_warnings') else 'ì—†ìŒ'}
"""

        # ì¢…í•© ë¶„ì„ ì„¹ì…˜
        synthesis_section = ""
        if research_synthesis:
            synthesis_section = f"""
## ğŸ“Š ìë£Œ ì¢…í•© ë¶„ì„
### ìë£Œë“¤ì´ ë™ì˜í•˜ëŠ” ì 
{json.dumps(research_synthesis.get('consensus_points', []), ensure_ascii=False)}

### ìë£Œ ê°„ ì˜ê²¬ ì°¨ì´
{json.dumps(research_synthesis.get('divergent_points', []), ensure_ascii=False)}

### ì •ë³´ ê²©ì°¨ (ì¶”ê°€ ì¡°ì‚¬ í•„ìš”)
{json.dumps(research_synthesis.get('information_gaps', []), ensure_ascii=False)}
"""

        prompt = f"""ë‹¹ì‹ ì€ íˆ¬ììœ„ì›íšŒì˜ Devil's Advocateì…ë‹ˆë‹¤.
{target_agent} ì—ì´ì „íŠ¸ì˜ ë¶„ì„ì— ë„ì „í•´ì•¼ í•©ë‹ˆë‹¤.

## ëŒ€ìƒ ë¶„ì„
- ì—ì´ì „íŠ¸: {target_agent}
- ì ìˆ˜: {target_score}/10
- ìš”ì•½: {target_summary}
- í•µì‹¬ í¬ì¸íŠ¸: {json.dumps(target_key_points, ensure_ascii=False)}

## ì£¼ì‹ ì •ë³´
{json.dumps(stock_info, ensure_ascii=False, indent=2) if stock_info else "ì •ë³´ ì—†ìŒ"}
{user_research_section}
{synthesis_section}
## í† ë¡  ë¼ìš´ë“œ: {round_number}/3 (ê°•ë„: {intensity})

---

ë‹¹ì‹ ì˜ ì—­í• :
1. ì´ ë¶„ì„ì˜ ì•½ì ê³¼ ë§¹ì ì„ ì°¾ìœ¼ì„¸ìš”
2. ì‹œì¥ì´ ì´ë¯¸ ì•Œê³  ìˆëŠ” ê²ƒê³¼ ìƒˆë¡œìš´ ì¸ì‚¬ì´íŠ¸ë¥¼ êµ¬ë¶„í•˜ì„¸ìš”
3. Pre-mortem: ì´ íˆ¬ìê°€ ì‹¤íŒ¨í•  ìˆ˜ ìˆëŠ” ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ì œì‹œí•˜ì„¸ìš”
4. ì—­ì‚¬ì  ì„ ë¡€ì™€ ìœ ì‚¬í•œ ì‹¤íŒ¨ ì‚¬ë¡€ë¥¼ ì°¾ìœ¼ì„¸ìš”
5. **ì‚¬ìš©ì ì œê³µ ìë£Œì—ì„œ í¸í–¥ì´ ë°œê²¬ëœ ê²½ìš°, ì´ë¥¼ ì§€ì í•˜ì„¸ìš”**
6. **ìë£Œ ê°„ ì˜ê²¬ ì°¨ì´ê°€ ìˆëŠ” ë¶€ë¶„ì— ëŒ€í•´ ì§ˆë¬¸í•˜ì„¸ìš”**

{f"Round {round_number}: ë” ë‚ ì¹´ë¡­ê³  êµ¬ì²´ì ì¸ ë„ì „ì„ í•˜ì„¸ìš”. ì´ì „ ë¼ìš´ë“œì—ì„œ í•´ê²°ë˜ì§€ ì•Šì€ ë¬¸ì œì— ì§‘ì¤‘í•˜ì„¸ìš”." if round_number > 1 else ""}

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:
{{
    "counter_argument": "í•µì‹¬ ë°˜ë°• ë…¼ë¦¬ (êµ¬ì²´ì ì´ê³  ë°ì´í„° ê¸°ë°˜ìœ¼ë¡œ, ìµœì†Œ 3ë¬¸ì¥)",
    "evidence": ["ë°˜ë°•ì„ ë’·ë°›ì¹¨í•˜ëŠ” ì¦ê±° 1", "ì¦ê±° 2", "ì¦ê±° 3"],
    "severity": "low" ë˜ëŠ” "medium" ë˜ëŠ” "high" ë˜ëŠ” "critical",
    "questions": ["ì´ ì—ì´ì „íŠ¸ê°€ ë‹µí•´ì•¼ í•  ì§ˆë¬¸ 1", "ì§ˆë¬¸ 2"],
    "blind_spots": ["ì´ ë¶„ì„ì´ ë†“ì¹œ ê²ƒ 1", "ë†“ì¹œ ê²ƒ 2"],
    "worst_case_scenario": "ìµœì•…ì˜ ì‹œë‚˜ë¦¬ì˜¤ ì„¤ëª…",
    "historical_parallel": "ìœ ì‚¬í•œ ì—­ì‚¬ì  ì‚¬ë¡€ (ìˆë‹¤ë©´)",
    "user_research_concerns": ["ì‚¬ìš©ì ìë£Œì—ì„œ ë°œê²¬ëœ ë¬¸ì œì  (ìˆë‹¤ë©´)"],
    "cited_sources": ["ì¸ìš©í•œ ì‚¬ìš©ì ìë£Œ ì œëª© (ìˆë‹¤ë©´)"]
}}"""

        try:
            response = await self.invoke(prompt)
            result = self._parse_json_response(response)

            return {
                "counter_argument": result.get("counter_argument", "ë„ì „ ìƒì„± ì‹¤íŒ¨"),
                "evidence": result.get("evidence", []),
                "severity": result.get("severity", "medium"),
                "questions": result.get("questions", []),
                "blind_spots": result.get("blind_spots", []),
                "worst_case": result.get("worst_case_scenario", ""),
                "historical_parallel": result.get("historical_parallel", ""),
            }

        except Exception as e:
            logger.error(f"Error generating challenge: {e}")
            return {
                "counter_argument": f"ë„ì „ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}",
                "evidence": [],
                "severity": "low",
                "questions": [],
            }

    async def rebut_defense(self, context: dict[str, Any]) -> dict[str, Any]:
        """ì—ì´ì „íŠ¸ì˜ ë°©ì–´ì— ëŒ€í•´ ì¬ë°˜ë°•í•©ë‹ˆë‹¤.

        Args:
            context:
                - original_challenge: ì›ë˜ ë„ì „ ë‚´ìš©
                - defense_response: ì—ì´ì „íŠ¸ì˜ ë°©ì–´ ì‘ë‹µ
                - acknowledged_risks: ì¸ì •ëœ ë¦¬ìŠ¤í¬
                - stock_info: ì£¼ì‹ ì •ë³´

        Returns:
            ì¬ë°˜ë°• ë‚´ìš©:
                - rebuttal: ì¬ë°˜ë°• ë…¼ë¦¬
                - remaining_concerns: ë‚¨ì€ ìš°ë ¤ ì‚¬í•­
                - accepted_defense: ë°›ì•„ë“¤ì¸ ë°©ì–´ í¬ì¸íŠ¸
        """
        original = context.get("original_challenge", "")
        defense = context.get("defense_response", "")
        acknowledged = context.get("acknowledged_risks", [])

        prompt = f"""ë‹¹ì‹ ì€ Devil's Advocateì…ë‹ˆë‹¤.
ì—ì´ì „íŠ¸ê°€ ë‹¹ì‹ ì˜ ë„ì „ì— ëŒ€í•´ ë°©ì–´í–ˆìŠµë‹ˆë‹¤. ì´ ë°©ì–´ë¥¼ í‰ê°€í•˜ì„¸ìš”.

## ì›ë˜ ë„ì „
{original}

## ì—ì´ì „íŠ¸ì˜ ë°©ì–´
{defense}

## ì¸ì •ëœ ë¦¬ìŠ¤í¬
{json.dumps(acknowledged, ensure_ascii=False)}

---

ì´ ë°©ì–´ê°€ ì¶©ë¶„í•œì§€ í‰ê°€í•˜ì„¸ìš”:
1. ë°©ì–´ê°€ ë…¼ë¦¬ì ì´ê³  ì¦ê±°ì— ê¸°ë°˜í–ˆëŠ”ê°€?
2. í•µì‹¬ ìš°ë ¤ì‚¬í•­ì´ í•´ì†Œë˜ì—ˆëŠ”ê°€?
3. ì¶”ê°€ë¡œ ì œê¸°í•´ì•¼ í•  ì ì´ ìˆëŠ”ê°€?

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:
{{
    "rebuttal": "ì¬ë°˜ë°• ë˜ëŠ” ìˆ˜ìš© ë©”ì‹œì§€",
    "defense_quality": "weak" ë˜ëŠ” "moderate" ë˜ëŠ” "strong",
    "remaining_concerns": ["í•´ì†Œë˜ì§€ ì•Šì€ ìš°ë ¤ 1", "ìš°ë ¤ 2"],
    "accepted_points": ["ì¸ì •í•˜ëŠ” ë°©ì–´ í¬ì¸íŠ¸ 1", "í¬ì¸íŠ¸ 2"],
    "final_verdict": "ë°©ì–´ê°€ ì¶©ë¶„í•¨" ë˜ëŠ” "ë¶€ë¶„ì ìœ¼ë¡œ ì¶©ë¶„í•¨" ë˜ëŠ” "ë°©ì–´ê°€ ë¶ˆì¶©ë¶„í•¨"
}}"""

        try:
            response = await self.invoke(prompt)
            result = self._parse_json_response(response)

            return {
                "rebuttal": result.get("rebuttal", ""),
                "defense_quality": result.get("defense_quality", "moderate"),
                "remaining_concerns": result.get("remaining_concerns", []),
                "accepted_points": result.get("accepted_points", []),
                "verdict": result.get("final_verdict", "ë¶€ë¶„ì ìœ¼ë¡œ ì¶©ë¶„í•¨"),
            }

        except Exception as e:
            logger.error(f"Error in rebut_defense: {e}")
            return {
                "rebuttal": "ì¬ë°˜ë°• ìƒì„± ì‹¤íŒ¨",
                "defense_quality": "unknown",
                "remaining_concerns": [],
                "accepted_points": [],
            }
